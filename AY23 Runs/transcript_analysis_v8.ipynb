{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba in /usr/local/lib/python3.10/dist-packages (0.11.2)\n",
      "Requirement already satisfied: coverage in /usr/local/lib/python3.10/dist-packages (from mamba) (7.2.3)\n",
      "Requirement already satisfied: clint in /usr/local/lib/python3.10/dist-packages (from mamba) (0.5.1)\n",
      "Requirement already satisfied: args in /usr/local/lib/python3.10/dist-packages (from clint->mamba) (0.1.0)\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.27.4)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (15.0.7)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scikit-network in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from scikit-network) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from scikit-network) (1.23.5)\n",
      "Collecting git+https://github.com/rwalk/gsdmm.git\n",
      "  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-rwckqsul\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-rwckqsul\n",
      "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gsdmm==0.1) (1.23.5)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.9.3)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.1\n",
      "2023-04-18 23:21:05.288197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 23:21:05.786423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-18 23:21:06.480893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-18 23:21:06.481234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-18 23:21:06.481401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Package installations to work on WIRE\n",
    "\n",
    "! pip install mamba\n",
    "! mamba install gensim openai -y\n",
    "! mamba install -c anaconda nltk -y\n",
    "! mamba install -c conda-forge spacy -y\n",
    "! mamba install -c conda-forge pyldavis -y\n",
    "! mamba install -c conda-forge pypdf2 -y\n",
    "! pip install transformers sentence-transformers\n",
    "! pip install scikit-network\n",
    "! pip install git+https://github.com/rwalk/gsdmm.git\n",
    "! pip install gensim\n",
    "\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-04-21 20:03:23.645303: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-21 20:03:24.185755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-21 20:03:24.722253: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-21 20:03:24.722284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 4c154600bf22\n",
      "2023-04-21 20:03:24.722293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 4c154600bf22\n",
      "2023-04-21 20:03:24.722383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.105.17\n",
      "2023-04-21 20:03:24.722400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.105.17\n",
      "2023-04-21 20:03:24.722405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.105.17\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import pickle\n",
    "import sknetwork as skn\n",
    "from random import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"names\", \"stopwords\", \"state_union\", \"twitter_samples\", \"movie_reviews\", \"averaged_perceptron_tagger\", \"vader_lexicon\", \"punkt\", \"wordnet\"])\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "# set seed for reproducibility\n",
    "# np.random.seed(493)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Text Cleaning and Topic Modeling (Same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokeniz(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "# function to filter out stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts\n",
    "    ]\n",
    "\n",
    "\n",
    "# function for lemmatization\n",
    "def lemmatize(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\"]):\n",
    "    texts_op = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_op.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_op\n",
    "\n",
    "\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    words = []\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(\n",
    "            cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True\n",
    "        )[:values]\n",
    "        words.append([w[0] for w in sort_dicts])\n",
    "        # print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        # print('-'*120)\n",
    "    words1 = deepcopy(words)\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(words[i])):\n",
    "            for k in range(len(words)):\n",
    "                if k != i and words[i][j] in words[k]:\n",
    "                    try:\n",
    "                        words1[i].remove(words[i][j])\n",
    "                        words1[k].remove(words[i][j])\n",
    "                    except:\n",
    "                        pass\n",
    "    return words1\n",
    "\n",
    "\n",
    "def create_topics_dataframe(data_text, mgp, threshold, topic_dict, stem_text):\n",
    "    result = pd.DataFrame(columns=[\"text\", \"topic\", \"stems\"])\n",
    "    for i, text in enumerate(data_text):\n",
    "        result.at[i, \"text\"] = text\n",
    "        result.at[i, \"stems\"] = stem_text[i]\n",
    "        prob = mgp.choose_best_label(stem_text[i])\n",
    "        if prob[1] >= threshold:\n",
    "            result.at[i, \"topic\"] = topic_dict[prob[0]]\n",
    "        else:\n",
    "            result.at[i, \"topic\"] = \"Other\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building Bigram & Trigram Models\n",
    "# bigram = gensim.models.Phrases(processed_data, min_count=5, threshold=100)\n",
    "# trigram = gensim.models.Phrases(bigram[processed_data], threshold=100)\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "# function to filter out stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts\n",
    "    ]\n",
    "\n",
    "\n",
    "# function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "# function to create trigrams\n",
    "def create_trigrams(texts):\n",
    "    [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "# function for lemmatization\n",
    "def lemmatize(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\"]):\n",
    "    texts_op = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_op.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Transcript-Topics (Same as Before)\n",
    "Final data saves with transcript, topic keywords, sentiment, and program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcript_cleaning(month):\n",
    "    #import spacy stuff\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    tr_tp_st = [[], [], [], []]\n",
    "\n",
    "    gl = pd.read_csv(\"foxGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"fox_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Sentence'] = lines['Line'].apply(lambda x: [str(sent) for sent in nlp(x).sents])\n",
    "            sentences = lines.explode('Sentence')\n",
    "            sentences = sentences.dropna(subset = ['Sentence'])\n",
    "            sentences[\"VADER\"] = sentences[\"Sentence\"].apply(\n",
    "                lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            )\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            words = words[0:15]\n",
    "            for w in words:\n",
    "                    try:\n",
    "                        w1 = w[0]\n",
    "                        wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n",
    "                        w_list.append(w1)\n",
    "                        sent_list = list(wdf[\"VADER\"])\n",
    "                        word_sentiment_list.append(sent_list)\n",
    "                    except:\n",
    "                        print('error with the contains line')\n",
    "\n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "\n",
    "    gl = pd.read_csv(\"msnbcGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"msnbc_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Sentence'] = lines['Line'].apply(lambda x: [str(sent) for sent in nlp(x).sents])\n",
    "            sentences = lines.explode('Sentence')\n",
    "            sentences = sentences.dropna(subset = ['Sentence'])\n",
    "            sentences[\"VADER\"] = sentences[\"Sentence\"].apply(\n",
    "                lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            )\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            words = words[0:15]\n",
    "            for w in words:\n",
    "                    try:\n",
    "                        w1 = w[0]\n",
    "                        wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n",
    "                        w_list.append(w1)\n",
    "                        sent_list = list(wdf[\"VADER\"])\n",
    "                        word_sentiment_list.append(sent_list)\n",
    "                    except:\n",
    "                        print('error with the contains line')\n",
    "\n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "    gl = pd.read_csv(\"CNNGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"cnn_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Sentence'] = lines['Line'].apply(lambda x: [str(sent) for sent in nlp(x).sents])\n",
    "            sentences = lines.explode('Sentence')\n",
    "            sentences = sentences.dropna(subset = ['Sentence'])\n",
    "            sentences[\"VADER\"] = sentences[\"Sentence\"].apply(\n",
    "                lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            )\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            words = words[0:15]\n",
    "            for w in words:\n",
    "                    try:\n",
    "                        w1 = w[0]\n",
    "                        wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n",
    "                        w_list.append(w1)\n",
    "                        sent_list = list(wdf[\"VADER\"])\n",
    "                        word_sentiment_list.append(sent_list)\n",
    "                    except:\n",
    "                        print('error with the contains line')\n",
    "\n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "    return tr_tp_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "90\n",
      "473\n",
      "501\n",
      "740\n",
      "832\n",
      "869\n",
      "1612\n",
      "2194\n",
      "2195\n",
      "2327\n",
      "2582\n",
      "2702\n",
      "2755\n",
      "2840\n",
      "430\n",
      "1400\n",
      "1409\n",
      "1560\n",
      "1632\n",
      "1756\n",
      "1866\n",
      "1869\n",
      "139\n",
      "600\n",
      "1505\n",
      "1599\n",
      "1698\n",
      "2065\n",
      "2225\n",
      "2368\n",
      "2759\n",
      "3226\n",
      "3327\n",
      "3680\n",
      "3683\n",
      "3689\n",
      "3704\n",
      "3708\n",
      "3714\n",
      "4261\n",
      "4299\n",
      "4625\n",
      "4681\n",
      "error with the contains line\n",
      "5183\n",
      "5395\n",
      "5785\n",
      "5854\n",
      "5881\n",
      "5885\n",
      "5933\n",
      "6293\n",
      "6633\n",
      "6789\n",
      "6998\n",
      "7092\n",
      "7108\n",
      "7124\n",
      "7155\n",
      "error with the contains line\n",
      "7237\n",
      "7743\n",
      "7830\n",
      "7844\n",
      "7941\n",
      "8555\n",
      "8690\n",
      "8697\n",
      "8831\n",
      "8929\n",
      "9102\n",
      "10037\n",
      "10043\n",
      "10420\n",
      "10659\n",
      "10727\n",
      "64\n",
      "347\n",
      "754\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "987\n",
      "1379\n",
      "2441\n",
      "2505\n",
      "2554\n",
      "449\n",
      "451\n",
      "456\n",
      "529\n",
      "531\n",
      "661\n",
      "764\n",
      "814\n",
      "950\n",
      "1385\n",
      "1852\n",
      "1855\n",
      "1856\n",
      "38\n",
      "69\n",
      "170\n",
      "201\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "252\n",
      "598\n",
      "error with the contains line\n",
      "1327\n",
      "1441\n",
      "1710\n",
      "2139\n",
      "2163\n",
      "2207\n",
      "2328\n",
      "3408\n",
      "3923\n",
      "3950\n",
      "4168\n",
      "4506\n",
      "4603\n",
      "4927\n",
      "5385\n",
      "5756\n",
      "5808\n",
      "5943\n",
      "6207\n",
      "6217\n",
      "6484\n",
      "6502\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "6757\n",
      "6769\n",
      "6889\n",
      "6931\n",
      "6939\n",
      "6954\n",
      "7001\n",
      "7069\n",
      "7189\n",
      "7322\n",
      "7456\n",
      "7460\n",
      "7476\n",
      "7479\n",
      "7621\n",
      "9814\n",
      "10444\n",
      "329\n",
      "371\n",
      "807\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "1653\n",
      "1929\n",
      "2449\n",
      "149\n",
      "163\n",
      "406\n",
      "error with the contains line\n",
      "869\n",
      "945\n",
      "1223\n",
      "1227\n",
      "1925\n",
      "33\n",
      "47\n",
      "102\n",
      "455\n",
      "722\n",
      "1052\n",
      "1057\n",
      "1366\n",
      "1445\n",
      "1638\n",
      "1800\n",
      "1802\n",
      "1871\n",
      "1900\n",
      "2535\n",
      "2550\n",
      "error with the contains line\n",
      "2632\n",
      "error with the contains line\n",
      "3380\n",
      "3449\n",
      "3451\n",
      "3974\n",
      "4764\n",
      "4941\n",
      "5424\n",
      "5475\n",
      "5923\n",
      "6487\n",
      "6758\n",
      "7070\n",
      "7298\n",
      "7307\n",
      "7403\n",
      "7578\n",
      "7629\n",
      "7636\n",
      "error with the contains line\n",
      "7888\n",
      "8096\n",
      "8250\n",
      "8268\n",
      "8439\n",
      "8523\n",
      "8540\n",
      "8543\n",
      "8789\n",
      "8979\n",
      "8993\n",
      "9029\n",
      "9567\n",
      "9651\n",
      "9883\n",
      "9972\n",
      "10505\n",
      "10633\n",
      "10747\n",
      "26\n",
      "450\n",
      "1011\n",
      "error with the contains line\n",
      "1084\n",
      "1183\n",
      "error with the contains line\n",
      "1444\n",
      "1585\n",
      "1643\n",
      "1646\n",
      "1701\n",
      "error with the contains line\n",
      "2530\n",
      "2615\n",
      "2674\n",
      "33\n",
      "654\n",
      "794\n",
      "1087\n",
      "1575\n",
      "1802\n",
      "364\n",
      "437\n",
      "553\n",
      "1233\n",
      "1728\n",
      "1735\n",
      "error with the contains line\n",
      "2536\n",
      "error with the contains line\n",
      "2676\n",
      "error with the contains line\n",
      "2860\n",
      "3094\n",
      "3095\n",
      "3375\n",
      "3498\n",
      "3600\n",
      "4044\n",
      "4372\n",
      "error with the contains line\n",
      "5069\n",
      "5138\n",
      "5485\n",
      "5511\n",
      "5586\n",
      "5759\n",
      "5811\n",
      "6028\n",
      "6303\n",
      "7344\n",
      "error with the contains line\n",
      "7549\n",
      "7550\n",
      "7673\n",
      "8529\n",
      "9255\n",
      "9262\n",
      "9613\n",
      "9711\n",
      "9837\n",
      "10025\n",
      "10044\n",
      "10097\n",
      "10266\n",
      "10346\n",
      "10671\n",
      "827\n",
      "1020\n",
      "2677\n",
      "171\n",
      "174\n",
      "184\n",
      "1480\n",
      "1494\n",
      "1496\n",
      "1695\n",
      "190\n",
      "239\n",
      "429\n",
      "430\n",
      "552\n",
      "1038\n",
      "1089\n",
      "1182\n",
      "1316\n",
      "1321\n",
      "1550\n",
      "1598\n",
      "1727\n",
      "1854\n",
      "2088\n",
      "2430\n",
      "2459\n",
      "2673\n",
      "3025\n",
      "3212\n",
      "3542\n",
      "3565\n",
      "3913\n",
      "3996\n",
      "error with the contains line\n",
      "4547\n",
      "4574\n",
      "4590\n",
      "4654\n",
      "5276\n",
      "error with the contains line\n",
      "5669\n",
      "5771\n",
      "error with the contains line\n",
      "6371\n",
      "6400\n",
      "6562\n",
      "6594\n",
      "6804\n",
      "6823\n",
      "error with the contains line\n",
      "7343\n",
      "7603\n",
      "7660\n",
      "8090\n",
      "8118\n",
      "8242\n",
      "8269\n",
      "8320\n",
      "8628\n",
      "9256\n",
      "9921\n",
      "10230\n",
      "10563\n",
      "283\n",
      "290\n",
      "717\n",
      "718\n",
      "723\n",
      "837\n",
      "1066\n",
      "1238\n",
      "1268\n",
      "1577\n",
      "1657\n",
      "1736\n",
      "2130\n",
      "2742\n",
      "144\n",
      "679\n",
      "1059\n",
      "1441\n",
      "1443\n",
      "1457\n",
      "1903\n",
      "1913\n",
      "1917\n",
      "72\n",
      "410\n",
      "905\n",
      "error with the contains line\n",
      "1377\n",
      "1556\n",
      "1663\n",
      "error with the contains line\n",
      "3273\n",
      "3275\n",
      "3291\n",
      "3294\n",
      "error with the contains line\n",
      "3712\n",
      "4047\n",
      "4088\n",
      "4351\n",
      "4485\n",
      "4605\n",
      "4659\n",
      "4770\n",
      "error with the contains line\n",
      "4987\n",
      "5175\n",
      "5356\n",
      "5411\n",
      "5520\n",
      "5526\n",
      "5539\n",
      "5873\n",
      "5997\n",
      "6001\n",
      "6003\n",
      "6011\n",
      "6377\n",
      "6517\n",
      "error with the contains line\n",
      "6795\n",
      "7195\n",
      "7384\n",
      "7711\n",
      "7781\n",
      "8139\n",
      "8231\n",
      "8594\n",
      "8610\n",
      "8895\n",
      "9132\n",
      "10251\n",
      "10313\n",
      "10400\n",
      "113\n",
      "236\n",
      "error with the contains line\n",
      "324\n",
      "845\n",
      "901\n",
      "1014\n",
      "error with the contains line\n",
      "1032\n",
      "1269\n",
      "1274\n",
      "error with the contains line\n",
      "1292\n",
      "1476\n",
      "1876\n",
      "1922\n",
      "1961\n",
      "error with the contains line\n",
      "2064\n",
      "257\n",
      "667\n",
      "1168\n",
      "1171\n",
      "1172\n",
      "1176\n",
      "1656\n",
      "1759\n",
      "1878\n",
      "2029\n",
      "36\n",
      "105\n",
      "224\n",
      "1087\n",
      "error with the contains line\n",
      "1249\n",
      "1313\n",
      "1353\n",
      "1484\n",
      "1723\n",
      "1773\n",
      "1880\n",
      "1956\n",
      "2556\n",
      "2874\n",
      "2893\n",
      "3128\n",
      "3227\n",
      "3254\n",
      "3577\n",
      "3587\n",
      "3621\n",
      "3626\n",
      "4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632/351936673.py:269: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4823\n",
      "4836\n",
      "error with the contains line\n",
      "5062\n",
      "error with the contains line\n",
      "5108\n",
      "5152\n",
      "5211\n",
      "5837\n",
      "6634\n",
      "6683\n",
      "error with the contains line\n",
      "7990\n",
      "8351\n",
      "8386\n",
      "8908\n",
      "8944\n",
      "9119\n",
      "9208\n",
      "9253\n",
      "9430\n",
      "9520\n",
      "9568\n",
      "9573\n",
      "9623\n",
      "9948\n",
      "9979\n",
      "9989\n",
      "10080\n",
      "10236\n",
      "10289\n",
      "10575\n",
      "465\n",
      "673\n",
      "1249\n",
      "1772\n",
      "1807\n",
      "2548\n",
      "7\n",
      "760\n",
      "810\n",
      "922\n",
      "1596\n",
      "2053\n",
      "1066\n",
      "1224\n",
      "error with the contains line\n",
      "1971\n",
      "2283\n",
      "2345\n",
      "2569\n",
      "error with the contains line\n",
      "2742\n",
      "2763\n",
      "2796\n",
      "3086\n",
      "error with the contains line\n",
      "error with the contains line\n",
      "3793\n",
      "3915\n",
      "3940\n",
      "4188\n",
      "4379\n",
      "4526\n",
      "4528\n",
      "4816\n",
      "4846\n",
      "5153\n",
      "5523\n",
      "5747\n",
      "5823\n",
      "6068\n",
      "6079\n",
      "6104\n",
      "6414\n",
      "6491\n",
      "6515\n",
      "6602\n",
      "6834\n",
      "6913\n",
      "7064\n",
      "7145\n",
      "7405\n",
      "7483\n",
      "7617\n",
      "7664\n",
      "8093\n",
      "error with the contains line\n",
      "8256\n",
      "8261\n",
      "8956\n",
      "9178\n",
      "9204\n",
      "9280\n",
      "9503\n",
      "9743\n",
      "10224\n",
      "209\n",
      "253\n",
      "374\n",
      "406\n",
      "1350\n",
      "1414\n",
      "1415\n",
      "1714\n",
      "1854\n",
      "1868\n",
      "1918\n",
      "1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632/351936673.py:177: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n",
      "error with the contains line\n",
      "720\n",
      "1302\n",
      "1777\n",
      "2001\n",
      "2111\n",
      "2114\n",
      "error with the contains line\n",
      "423\n",
      "444\n",
      "651\n",
      "657\n",
      "876\n",
      "1080\n",
      "1164\n",
      "1253\n",
      "2003\n",
      "2770\n",
      "3360\n",
      "3366\n",
      "3551\n",
      "3798\n",
      "4056\n",
      "4415\n",
      "5559\n",
      "5564\n",
      "6204\n",
      "6305\n",
      "6483\n",
      "7321\n",
      "7698\n",
      "7702\n",
      "7865\n",
      "8352\n",
      "8488\n",
      "8506\n",
      "8807\n",
      "8866\n",
      "9082\n",
      "9136\n",
      "9194\n",
      "9323\n",
      "9401\n",
      "9416\n",
      "9680\n",
      "9946\n",
      "10217\n",
      "10737\n",
      "68\n",
      "error with the contains line\n",
      "553\n",
      "572\n",
      "628\n",
      "653\n",
      "721\n",
      "883\n",
      "897\n",
      "error with the contains line\n",
      "956\n",
      "error with the contains line\n",
      "1173\n",
      "1486\n",
      "1500\n",
      "2003\n",
      "2137\n",
      "2198\n",
      "2629\n",
      "2819\n",
      "267\n",
      "335\n",
      "767\n",
      "error with the contains line\n",
      "1268\n",
      "1269\n",
      "1533\n",
      "1776\n",
      "1973\n",
      "2041\n",
      "2086\n",
      "2087\n",
      "93\n",
      "335\n",
      "401\n",
      "564\n",
      "706\n",
      "855\n",
      "1216\n",
      "1384\n",
      "1588\n",
      "2278\n",
      "2284\n",
      "2290\n",
      "2295\n",
      "2534\n",
      "2670\n",
      "2895\n",
      "3014\n",
      "3072\n",
      "3893\n",
      "3895\n",
      "4017\n",
      "4081\n",
      "error with the contains line\n",
      "5254\n",
      "5647\n",
      "5692\n",
      "5762\n",
      "5792\n",
      "5897\n",
      "6141\n",
      "6205\n",
      "6358\n",
      "6382\n",
      "6413\n",
      "6465\n",
      "6466\n",
      "6564\n",
      "7772\n",
      "7912\n",
      "8149\n",
      "8188\n",
      "8221\n",
      "8282\n",
      "8611\n",
      "8637\n",
      "8656\n",
      "8678\n",
      "8771\n",
      "8864\n",
      "9161\n",
      "9176\n",
      "9189\n",
      "9660\n",
      "9805\n",
      "52\n",
      "55\n",
      "89\n",
      "277\n",
      "561\n",
      "629\n",
      "732\n",
      "804\n",
      "error with the contains line\n",
      "963\n",
      "1119\n",
      "1144\n",
      "1347\n",
      "1351\n",
      "error with the contains line\n",
      "1505\n",
      "1705\n",
      "1733\n",
      "2254\n",
      "2829\n",
      "189\n",
      "309\n",
      "323\n",
      "770\n",
      "1030\n",
      "1511\n",
      "error with the contains line\n",
      "147\n",
      "639\n",
      "649\n",
      "656\n",
      "698\n",
      "738\n",
      "795\n",
      "816\n",
      "833\n",
      "882\n",
      "883\n",
      "1207\n",
      "1379\n",
      "1406\n",
      "1537\n",
      "1990\n",
      "2702\n",
      "2836\n",
      "3165\n",
      "3310\n",
      "3528\n",
      "4365\n",
      "4875\n",
      "5574\n",
      "5734\n",
      "6080\n",
      "7454\n",
      "7875\n",
      "8494\n",
      "8592\n",
      "8593\n",
      "8717\n",
      "9390\n",
      "9533\n",
      "9808\n",
      "10152\n",
      "10371\n",
      "error with the contains line\n",
      "483\n",
      "error with the contains line\n",
      "773\n",
      "824\n",
      "838\n",
      "854\n",
      "1213\n",
      "1395\n",
      "2113\n",
      "2175\n",
      "2566\n",
      "295\n",
      "545\n",
      "1112\n",
      "1113\n",
      "1837\n",
      "1838\n"
     ]
    }
   ],
   "source": [
    "# Save out processed and topic modeled data\n",
    "months = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "for m in months:\n",
    "    tr_tp_st = transcript_cleaning(m)\n",
    "    tdf = pd.DataFrame(zip(tr_tp_st[0], tr_tp_st[1], tr_tp_st[2], tr_tp_st[3]), \n",
    "                       columns=[\"Transcript\", \"Topic\", \"Sentiment\", \"Program\"],)\n",
    "    with open('new processed_transcripts_' + m + '2020.pkl', 'wb') as file:\n",
    "        pickle.dump(tr_tp_st, file)\n",
    "    tdf.to_csv('new processed_transcripts_' + m + '2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 21, 21, 21, 21]\n",
      "[['generation', 'free', 'vote', 'people', 'man', 'feel', 'see'], ['know', 'word', 'impeachment', 'election', 'evidence', 'use'], ['stage', 'mueller', 'grief', 'question', 'country'], ['information', 'president', 'memo', 'medium', 'leak', 'news', 'receive'], ['tonight', 'liberal', 'way', 'angle', 'good']]\n",
      "[0.1352142857142857, 0.004306896551724135, -0.026888000000000006, -0.18042962962962963, -0.03984999999999999]\n",
      "['Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE']\n",
      "6571\n",
      "MSNBC 11TH HOUR WITH BRIAN WILLIAMS\n"
     ]
    }
   ],
   "source": [
    "print(tr_tp_st[0][:5])\n",
    "print(tr_tp_st[1][:5])\n",
    "print(tr_tp_st[2][:5])\n",
    "print(tr_tp_st[3][:5])\n",
    "print(len(tr_tp_st[1]))\n",
    "print(tr_tp_st[3][1440])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
