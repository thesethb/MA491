{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba in /usr/local/lib/python3.10/dist-packages (0.11.2)\n",
      "Requirement already satisfied: coverage in /usr/local/lib/python3.10/dist-packages (from mamba) (7.2.3)\n",
      "Requirement already satisfied: clint in /usr/local/lib/python3.10/dist-packages (from mamba) (0.5.1)\n",
      "Requirement already satisfied: args in /usr/local/lib/python3.10/dist-packages (from clint->mamba) (0.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.27.4)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (15.0.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-network in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from scikit-network) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from scikit-network) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/rwalk/gsdmm.git\n",
      "  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-ezifjbpr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-ezifjbpr\n",
      "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gsdmm==0.1) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-04-22 17:35:38.249497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 17:35:38.748793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-22 17:35:39.451063: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-22 17:35:39.451095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 4c154600bf22\n",
      "2023-04-22 17:35:39.451102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 4c154600bf22\n",
      "2023-04-22 17:35:39.451139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.105.17\n",
      "2023-04-22 17:35:39.451158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.105.17\n",
      "2023-04-22 17:35:39.451166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.105.17\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (59.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Package installations to work on WIRE\n",
    "\n",
    "! pip install mamba\n",
    "! mamba install gensim openai -y\n",
    "! mamba install -c anaconda nltk -y\n",
    "! mamba install -c conda-forge spacy -y\n",
    "! mamba install -c conda-forge pyldavis -y\n",
    "! mamba install -c conda-forge pypdf2 -y\n",
    "! pip install transformers sentence-transformers\n",
    "! pip install scikit-network\n",
    "! pip install git+https://github.com/rwalk/gsdmm.git\n",
    "! pip install gensim\n",
    "\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-04-22 17:35:45.803835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 17:35:46.307632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-22 17:35:46.794887: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-22 17:35:46.794915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 4c154600bf22\n",
      "2023-04-22 17:35:46.794921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 4c154600bf22\n",
      "2023-04-22 17:35:46.795016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.105.17\n",
      "2023-04-22 17:35:46.795030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.105.17\n",
      "2023-04-22 17:35:46.795036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.105.17\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import pickle\n",
    "import sknetwork as skn\n",
    "from random import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"names\", \"stopwords\", \"state_union\", \"twitter_samples\", \"movie_reviews\", \"averaged_perceptron_tagger\", \"vader_lexicon\", \"punkt\", \"wordnet\"])\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "# set seed for reproducibility\n",
    "# np.random.seed(493)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Transcript-Topics (Same as Before)\n",
    "Final data saves with transcript, topic keywords, sentiment, and program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcript_cleaning(month):\n",
    "    #import spacy stuff\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    tr_tp_st = [[], [], [], []]\n",
    "\n",
    "    gl = pd.read_csv(\"foxGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"fox_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Line'] = lines['Line'].apply(lambda x: x.lower())\n",
    "            lines[\"VADER\"] = lines[\"Line\"].apply(\n",
    "                lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            )\n",
    "\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            tokens = nltk.word_tokenize(raw_text)\n",
    "            words = Counter(tokens).most_common()\n",
    "            words1 = []\n",
    "            i = 0\n",
    "            while len(words1) < 15 and i < len(words):\n",
    "                w = words[i][0]\n",
    "                #print(w)\n",
    "                tag =  nltk.pos_tag([w])\n",
    "                t = tag[0][1] \n",
    "                if t in ['NN','NNS','NNP','NNPS','FW','SYM'] and w not in ['video','clip','i','end','begin']: #or t == \"FW\":\n",
    "                    words1.append(w)\n",
    "                i += 1\n",
    "\n",
    "            for w in words1:\n",
    "                try:\n",
    "                    w1 = w\n",
    "                    wdf = lines[lines['Line'].str.contains(w1)]\n",
    "                    w_list.append(w1)\n",
    "                    sent_list = list(wdf[\"VADER\"])\n",
    "                    word_sentiment_list.append(sent_list)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "    gl = pd.read_csv(\"msnbcGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"msnbc_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Line'] = lines['Line'].apply(lambda x: x.lower())\n",
    "            lines[\"VADER\"] = lines[\"Line\"].apply(\n",
    "                lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            )\n",
    "\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            tokens = nltk.word_tokenize(raw_text)\n",
    "            words = Counter(tokens).most_common()\n",
    "            words1 = []\n",
    "            i = 0\n",
    "            while len(words1) < 15 and i < len(words):\n",
    "                w = words[i][0]\n",
    "                #print(w)\n",
    "                tag =  nltk.pos_tag([w])\n",
    "                t = tag[0][1] \n",
    "                if t in ['NN','NNS','NNP','NNPS','FW','SYM'] and w not in ['video','clip','i','end','begin']: #or t == \"FW\":\n",
    "                    words1.append(w)\n",
    "                i += 1\n",
    "\n",
    "            for w in words1:\n",
    "                try:\n",
    "                    w1 = w\n",
    "                    wdf = lines[lines['Line'].str.contains(w1)]\n",
    "                    w_list.append(w1)\n",
    "                    sent_list = list(wdf[\"VADER\"])\n",
    "                    word_sentiment_list.append(sent_list)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "\n",
    "    gl = pd.read_csv(\"CNNGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"cnn_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Line'] = lines['Line'].apply(lambda x: x.lower())\n",
    "            lines[\"VADER\"] = lines[\"Line\"].apply(\n",
    "                lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            )\n",
    "\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            tokens = nltk.word_tokenize(raw_text)\n",
    "            words = Counter(tokens).most_common()\n",
    "            words1 = []\n",
    "            i = 0\n",
    "            while len(words1) < 15 and i < len(words):\n",
    "                w = words[i][0]\n",
    "                #print(w)\n",
    "                tag =  nltk.pos_tag([w])\n",
    "                t = tag[0][1] \n",
    "                if t in ['NN','NNS','NNP','NNPS','FW','SYM'] and w not in ['video','clip','i','end','begin']: #or t == \"FW\":\n",
    "                    words1.append(w)\n",
    "                i += 1\n",
    "\n",
    "            for w in words1:\n",
    "                try:\n",
    "                    w1 = w\n",
    "                    wdf = lines[lines['Line'].str.contains(w1)]\n",
    "                    w_list.append(w1)\n",
    "                    sent_list = list(wdf[\"VADER\"])\n",
    "                    word_sentiment_list.append(sent_list)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "    return tr_tp_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "496\n",
      "859\n",
      "891\n",
      "1286\n",
      "1295\n",
      "1612\n",
      "1798\n",
      "1859\n",
      "2293\n",
      "2451\n",
      "2675\n",
      "2755\n",
      "827\n",
      "932\n",
      "1407\n",
      "1628\n",
      "1631\n",
      "2130\n",
      "233\n",
      "403\n",
      "881\n",
      "974\n",
      "1746\n",
      "1783\n",
      "2121\n",
      "2132\n",
      "2225\n",
      "2236\n",
      "2305\n",
      "2363\n",
      "2642\n",
      "3011\n",
      "3554\n",
      "3682\n",
      "3684\n",
      "3746\n",
      "3854\n",
      "3861\n",
      "3871\n",
      "3890\n",
      "4252\n",
      "4615\n",
      "4622\n",
      "4865\n",
      "5205\n",
      "5885\n",
      "5887\n",
      "5891\n",
      "5933\n",
      "6037\n",
      "6039\n",
      "6420\n",
      "6697\n",
      "7011\n",
      "7031\n",
      "7243\n",
      "7300\n",
      "7412\n",
      "7751\n",
      "7753\n",
      "7850\n",
      "8129\n",
      "8248\n",
      "9191\n",
      "9943\n",
      "10179\n",
      "10184\n",
      "10195\n",
      "10786\n",
      "10819\n",
      "544\n",
      "908\n",
      "938\n",
      "1499\n",
      "1633\n",
      "2620\n",
      "2695\n",
      "84\n",
      "85\n",
      "447\n",
      "531\n",
      "783\n",
      "820\n",
      "1008\n",
      "1611\n",
      "1847\n",
      "1851\n",
      "728\n",
      "896\n",
      "928\n",
      "1107\n",
      "1214\n",
      "1325\n",
      "1501\n",
      "1516\n",
      "1691\n",
      "1917\n",
      "2159\n",
      "2197\n",
      "2227\n",
      "2435\n",
      "2530\n",
      "2663\n",
      "3225\n",
      "3559\n",
      "4119\n",
      "4133\n",
      "4493\n",
      "4508\n",
      "4830\n",
      "5338\n",
      "5862\n",
      "5906\n",
      "6146\n",
      "6162\n",
      "6755\n",
      "6782\n",
      "6945\n",
      "6993\n",
      "7004\n",
      "7111\n",
      "7764\n",
      "8166\n",
      "8524\n",
      "9942\n",
      "10374\n",
      "182\n",
      "203\n",
      "226\n",
      "399\n",
      "1047\n",
      "2353\n",
      "2522\n",
      "2553\n",
      "147\n",
      "433\n",
      "875\n",
      "906\n",
      "975\n",
      "1213\n",
      "1466\n",
      "1475\n",
      "1922\n",
      "1931\n",
      "186\n",
      "293\n",
      "336\n",
      "611\n",
      "865\n",
      "1459\n",
      "1708\n",
      "1722\n",
      "1826\n",
      "1843\n",
      "1995\n",
      "3047\n",
      "3641\n",
      "3863\n",
      "3962\n",
      "4342\n",
      "5560\n",
      "5626\n",
      "5846\n",
      "5955\n",
      "6136\n",
      "6137\n",
      "6151\n",
      "6857\n",
      "6932\n",
      "6943\n",
      "7012\n",
      "7269\n",
      "7289\n",
      "7765\n",
      "8297\n",
      "8617\n",
      "9269\n",
      "9560\n",
      "9679\n",
      "10354\n",
      "10422\n",
      "10780\n",
      "390\n",
      "1005\n",
      "1281\n",
      "1723\n",
      "2637\n",
      "2746\n",
      "642\n",
      "644\n",
      "790\n",
      "801\n",
      "1089\n",
      "1580\n",
      "1585\n",
      "1787\n",
      "1804\n",
      "1159\n",
      "1228\n",
      "1232\n",
      "1815\n",
      "2008\n",
      "2080\n",
      "2471\n",
      "2581\n",
      "2614\n",
      "2676\n",
      "2767\n",
      "2777\n",
      "2860\n",
      "2889\n",
      "3250\n",
      "3586\n",
      "4087\n",
      "4127\n",
      "4589\n",
      "4917\n",
      "5106\n",
      "5458\n",
      "5483\n",
      "5512\n",
      "5592\n",
      "5615\n",
      "5660\n",
      "6027\n",
      "6336\n",
      "7040\n",
      "7740\n",
      "8529\n",
      "8563\n",
      "8718\n",
      "9454\n",
      "9455\n",
      "9662\n",
      "9717\n",
      "9896\n",
      "10094\n",
      "10121\n",
      "10225\n",
      "10267\n",
      "10346\n",
      "10573\n",
      "10757\n",
      "10781\n",
      "10800\n",
      "111\n",
      "189\n",
      "255\n",
      "522\n",
      "976\n",
      "1467\n",
      "1470\n",
      "1679\n",
      "1684\n",
      "2246\n",
      "2264\n",
      "2393\n",
      "2429\n",
      "2489\n",
      "172\n",
      "181\n",
      "183\n",
      "184\n",
      "412\n",
      "700\n",
      "708\n",
      "711\n",
      "978\n",
      "1026\n",
      "1029\n",
      "1248\n",
      "1493\n",
      "1696\n",
      "1699\n",
      "1704\n",
      "1941\n",
      "142\n",
      "239\n",
      "249\n",
      "349\n",
      "360\n",
      "473\n",
      "663\n",
      "1237\n",
      "1290\n",
      "1550\n",
      "1838\n",
      "1848\n",
      "1974\n",
      "2869\n",
      "3270\n",
      "3629\n",
      "3996\n",
      "4738\n",
      "4804\n",
      "5072\n",
      "5125\n",
      "5405\n",
      "5678\n",
      "5807\n",
      "6354\n",
      "6428\n",
      "7297\n",
      "7340\n",
      "7583\n",
      "8451\n",
      "8931\n",
      "8947\n",
      "8966\n",
      "9059\n",
      "9237\n",
      "9693\n",
      "9705\n",
      "10411\n",
      "10568\n",
      "10719\n",
      "4\n",
      "301\n",
      "745\n",
      "1177\n",
      "1203\n",
      "1403\n",
      "1446\n",
      "1824\n",
      "2022\n",
      "2081\n",
      "2368\n",
      "2515\n",
      "2733\n",
      "682\n",
      "898\n",
      "1198\n",
      "1438\n",
      "1448\n",
      "1452\n",
      "1763\n",
      "1916\n",
      "23\n",
      "398\n",
      "479\n",
      "1624\n",
      "1648\n",
      "1738\n",
      "3022\n",
      "3144\n",
      "3283\n",
      "3302\n",
      "3303\n",
      "4435\n",
      "4707\n",
      "4871\n",
      "4977\n",
      "4988\n",
      "4998\n",
      "5051\n",
      "5053\n",
      "5172\n",
      "5260\n",
      "5596\n",
      "5718\n",
      "5770\n",
      "5868\n",
      "6002\n",
      "6289\n",
      "6509\n",
      "6540\n",
      "6556\n",
      "6655\n",
      "6712\n",
      "6801\n",
      "7117\n",
      "7310\n",
      "7475\n",
      "7626\n",
      "7894\n",
      "7938\n",
      "7946\n",
      "8038\n",
      "8231\n",
      "8452\n",
      "8708\n",
      "8810\n",
      "9101\n",
      "9214\n",
      "9357\n",
      "9488\n",
      "9608\n",
      "9821\n",
      "9822\n",
      "10023\n",
      "10027\n",
      "353\n",
      "440\n",
      "564\n",
      "649\n",
      "800\n",
      "1014\n",
      "1400\n",
      "2080\n",
      "2430\n",
      "2766\n",
      "2772\n",
      "2844\n",
      "119\n",
      "249\n",
      "674\n",
      "1013\n",
      "1429\n",
      "1655\n",
      "1892\n",
      "407\n",
      "415\n",
      "898\n",
      "1068\n",
      "1147\n",
      "1155\n",
      "1353\n",
      "2634\n",
      "2831\n",
      "2893\n",
      "3077\n",
      "3134\n",
      "3138\n",
      "3154\n",
      "3465\n",
      "3970\n",
      "4920\n",
      "5012\n",
      "5041\n",
      "5266\n",
      "5731\n",
      "5736\n",
      "6164\n",
      "6166\n",
      "7641\n",
      "7792\n",
      "7950\n",
      "8142\n",
      "8349\n",
      "8375\n",
      "8390\n",
      "8392\n",
      "8808\n",
      "8853\n",
      "9014\n",
      "9033\n",
      "9035\n",
      "9629\n",
      "9648\n",
      "9741\n",
      "9919\n",
      "9988\n",
      "10084\n",
      "10164\n",
      "10200\n",
      "10287\n",
      "10398\n",
      "10655\n",
      "92\n",
      "221\n",
      "285\n",
      "411\n",
      "412\n",
      "507\n",
      "596\n",
      "737\n",
      "1695\n",
      "1770\n",
      "1935\n",
      "2280\n",
      "2812\n",
      "39\n",
      "49\n",
      "274\n",
      "501\n",
      "757\n",
      "760\n",
      "761\n",
      "1097\n",
      "1347\n",
      "351\n",
      "677\n",
      "678\n",
      "709\n",
      "893\n",
      "1224\n",
      "1297\n",
      "2075\n",
      "2202\n",
      "2218\n",
      "2279\n",
      "2335\n",
      "2575\n",
      "2578\n",
      "3776\n",
      "3792\n",
      "3819\n",
      "3936\n",
      "4543\n",
      "5208\n",
      "5355\n",
      "5375\n",
      "6012\n",
      "6259\n",
      "6625\n",
      "6821\n",
      "6831\n",
      "6842\n",
      "6868\n",
      "7128\n",
      "7285\n",
      "7616\n",
      "8404\n",
      "8489\n",
      "8573\n",
      "8605\n",
      "8830\n",
      "9040\n",
      "9315\n",
      "9445\n",
      "9503\n",
      "9581\n",
      "9740\n",
      "10073\n",
      "406\n",
      "506\n",
      "538\n",
      "1803\n",
      "2038\n",
      "2669\n",
      "2730\n",
      "362\n",
      "371\n",
      "378\n",
      "379\n",
      "386\n",
      "510\n",
      "604\n",
      "724\n",
      "988\n",
      "1556\n",
      "1780\n",
      "2006\n",
      "2114\n",
      "260\n",
      "509\n",
      "534\n",
      "1048\n",
      "1210\n",
      "1257\n",
      "2447\n",
      "2686\n",
      "2814\n",
      "2897\n",
      "3039\n",
      "3060\n",
      "3207\n",
      "3552\n",
      "3703\n",
      "3972\n",
      "4384\n",
      "4552\n",
      "4876\n",
      "5752\n",
      "6132\n",
      "6568\n",
      "6704\n",
      "6791\n",
      "7457\n",
      "7692\n",
      "7961\n",
      "8216\n",
      "8317\n",
      "8521\n",
      "8710\n",
      "8730\n",
      "8861\n",
      "9136\n",
      "9152\n",
      "9209\n",
      "9244\n",
      "9323\n",
      "9375\n",
      "9577\n",
      "9908\n",
      "10119\n",
      "10430\n",
      "571\n",
      "708\n",
      "727\n",
      "898\n",
      "956\n",
      "1328\n",
      "1330\n",
      "1428\n",
      "1831\n",
      "1837\n",
      "214\n",
      "356\n",
      "1280\n",
      "1528\n",
      "1981\n",
      "2122\n",
      "26\n",
      "29\n",
      "215\n",
      "525\n",
      "855\n",
      "858\n",
      "956\n",
      "1110\n",
      "1483\n",
      "1966\n",
      "1989\n",
      "2286\n",
      "2331\n",
      "2347\n",
      "2380\n",
      "2644\n",
      "2734\n",
      "3732\n",
      "4714\n",
      "5115\n",
      "5418\n",
      "5692\n",
      "5788\n",
      "5907\n",
      "5991\n",
      "6141\n",
      "6170\n",
      "6343\n",
      "6743\n",
      "6753\n",
      "6771\n",
      "7958\n",
      "8011\n",
      "8284\n",
      "8319\n",
      "8536\n",
      "8598\n",
      "8634\n",
      "9176\n",
      "9750\n",
      "9833\n",
      "9939\n",
      "10123\n",
      "10459\n",
      "10570\n",
      "10609\n",
      "18\n",
      "28\n",
      "59\n",
      "131\n",
      "563\n",
      "629\n",
      "707\n",
      "1119\n",
      "1122\n",
      "2107\n",
      "2187\n",
      "2267\n",
      "2593\n",
      "2595\n",
      "2691\n",
      "308\n",
      "310\n",
      "311\n",
      "321\n",
      "323\n",
      "397\n",
      "512\n",
      "566\n",
      "567\n",
      "571\n",
      "630\n",
      "632\n",
      "1504\n",
      "1955\n",
      "1962\n",
      "2032\n",
      "2125\n",
      "281\n",
      "301\n",
      "683\n",
      "736\n",
      "749\n",
      "1386\n",
      "1475\n",
      "1503\n",
      "1524\n",
      "1980\n",
      "2702\n",
      "2845\n",
      "3150\n",
      "3310\n",
      "3370\n",
      "3471\n",
      "3566\n",
      "3623\n",
      "4031\n",
      "4385\n",
      "4418\n",
      "4427\n",
      "4599\n",
      "4757\n",
      "5165\n",
      "6639\n",
      "6864\n",
      "7594\n",
      "7701\n",
      "7771\n",
      "7887\n",
      "8083\n",
      "8674\n",
      "8766\n",
      "9585\n",
      "10056\n",
      "10159\n",
      "10383\n",
      "77\n",
      "100\n",
      "158\n",
      "173\n",
      "420\n",
      "526\n",
      "773\n",
      "1396\n",
      "1625\n",
      "1688\n",
      "2126\n",
      "2205\n",
      "2848\n",
      "54\n",
      "56\n",
      "390\n",
      "1005\n",
      "1122\n",
      "1837\n",
      "2068\n"
     ]
    }
   ],
   "source": [
    "# Save out processed and topic modeled data\n",
    "months = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "for m in months:\n",
    "    tr_tp_st = transcript_cleaning(m)\n",
    "    tdf = pd.DataFrame(zip(tr_tp_st[0], tr_tp_st[1], tr_tp_st[2], tr_tp_st[3]), \n",
    "                       columns=[\"Transcript\", \"Topic\", \"Sentiment\", \"Program\"],)\n",
    "    with open('v9 processed_transcripts_' + m + '2020.pkl', 'wb') as file:\n",
    "        pickle.dump(tr_tp_st, file)\n",
    "    tdf.to_csv('v9 processed_transcripts_' + m + '2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 21, 21, 21, 21]\n",
      "[['generation', 'free', 'vote', 'people', 'man', 'feel', 'see'], ['know', 'word', 'impeachment', 'election', 'evidence', 'use'], ['stage', 'mueller', 'grief', 'question', 'country'], ['information', 'president', 'memo', 'medium', 'leak', 'news', 'receive'], ['tonight', 'liberal', 'way', 'angle', 'good']]\n",
      "[0.1352142857142857, 0.004306896551724135, -0.026888000000000006, -0.18042962962962963, -0.03984999999999999]\n",
      "['Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE', 'Fox News Network INGRAHAM ANGLE']\n",
      "6571\n",
      "MSNBC 11TH HOUR WITH BRIAN WILLIAMS\n"
     ]
    }
   ],
   "source": [
    "print(tr_tp_st[0][:5])\n",
    "print(tr_tp_st[1][:5])\n",
    "print(tr_tp_st[2][:5])\n",
    "print(tr_tp_st[3][:5])\n",
    "print(len(tr_tp_st[1]))\n",
    "print(tr_tp_st[3][1440])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
