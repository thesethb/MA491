{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/jovyan/.local/lib/python3.10/site-packages (0.0.158)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.13)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.30.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jovyan/.local/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/jovyan/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/jovyan/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/jovyan/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.3->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Package installations to work on WIRE\n",
    "\n",
    "! pip install gensim\n",
    "! pip install nltk\n",
    "! pip install sentence_transformers\n",
    "! pip install spacy\n",
    "! python3 -m spacy download en_core_web_sm\n",
    "! pip install bertopic\n",
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package names to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Seth Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import sys\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance, LangChain, TextGeneration\n",
    "from hdbscan import HDBSCAN\n",
    "import umap\n",
    "\n",
    "import openai\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain, HuggingFacePipeline, FewShotPromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "from random import *\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "nltk.download([\"names\", \"stopwords\", \"state_union\", \"twitter_samples\", \"movie_reviews\", \"averaged_perceptron_tagger\", \"vader_lexicon\", \"punkt\", \"wordnet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LLM Model\n",
    "- Create the Prompt\n",
    "- Create an LLM (more than one option)\n",
    "    - using a local LLM via LangChain's interface to HuggingFace\n",
    "    - using GPT-4 as a chat completion through Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a generic prompt template\n",
    "\n",
    "template = '''\n",
    "The following is a sentence from a cable news transcript. For context, the previous sentence was \"{previous}\"\n",
    "\n",
    "Please classify whether the sentence is POSITIVE, NEUTRAL, or NEGATIVE towards \"{subject}\". Only return the stance.\n",
    "\n",
    "sentence: {statement}\n",
    "'''\n",
    "\n",
    "stance_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\",\"statement\",\"previous\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's interface to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869427d82d9f4be69730fd4f2ecfc82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the LLM to use, Langchain verison\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"declare-lab/flan-alpaca-gpt4-xl\", task = 'text2text-generation', device=0,\n",
    "                                      model_kwargs={\"max_length\":500, \"no_repeat_ngram_size\":2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4 as a chat completion through Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"aci_openai_credentials.txt\") as f:\n",
    "    open_api_base, open_api_key = f.read().split(\"\\n\")\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = open_api_base\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = open_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'previous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m      2\u001b[0m   engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mACI_GPT-4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m----> 3\u001b[0m   messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m:stance_prompt\u001b[39m.\u001b[39mformat(subject \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDonald Trump\u001b[39m\u001b[39m\"\u001b[39m, statement\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThose fires - Trump Didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt do that, ANTIFA did!\u001b[39m\u001b[39m\"\u001b[39m)}],\n\u001b[0;32m      4\u001b[0m   temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,\n\u001b[0;32m      5\u001b[0m   max_tokens\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m,\n\u001b[0;32m      6\u001b[0m   top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m,\n\u001b[0;32m      7\u001b[0m   frequency_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m      8\u001b[0m   presence_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m      9\u001b[0m   stop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\prompts\\prompt.py:116\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39m        prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_partial_and_user_variables(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 116\u001b[0m \u001b[39mreturn\u001b[39;00m DEFAULT_FORMATTER_MAPPING[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate_format](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[1;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat\u001b[39m(\u001b[39mself\u001b[39m, format_string, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvformat(format_string, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\utils\\formatting.py:29\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo arguments should be provided, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meverything should be passed as keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     )\n\u001b[1;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mvformat(format_string, args, kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvformat\u001b[39m(\u001b[39mself\u001b[39m, format_string, args, kwargs):\n\u001b[0;32m    193\u001b[0m     used_args \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m--> 194\u001b[0m     result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vformat(format_string, args, kwargs, used_args, \u001b[39m2\u001b[39m)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[0;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[1;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[0;32m    230\u001b[0m     auto_arg_index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m# given the field_name, find the object it references\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m#  and the argument it came from\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m obj, arg_used \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_field(field_name, args, kwargs)\n\u001b[0;32m    235\u001b[0m used_args\u001b[39m.\u001b[39madd(arg_used)\n\u001b[0;32m    237\u001b[0m \u001b[39m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[1;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_field\u001b[39m(\u001b[39mself\u001b[39m, field_name, args, kwargs):\n\u001b[0;32m    297\u001b[0m     first, rest \u001b[39m=\u001b[39m _string\u001b[39m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[1;32m--> 299\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_value(first, args, kwargs)\n\u001b[0;32m    301\u001b[0m     \u001b[39m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[39m#  getattr or getitem as needed\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[39mfor\u001b[39;00m is_attr, i \u001b[39min\u001b[39;00m rest:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[1;34m(self, key, args, kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m args[key]\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m kwargs[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'previous'"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  engine=\"ACI_GPT-4\",\n",
    "  messages = [{\"role\":\"user\",\"content\":stance_prompt.format(subject = \"Donald Trump\", statement=\"Those fires - Trump Didn't do that, ANTIFA did!\")}],\n",
    "  temperature=0.7,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEUTRAL'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Transcript-Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set up llm df\n",
    "llm_df = pd.DataFrame(columns = ['subject','sentence','output'])\n",
    "def transcript_cleaning(month):\n",
    "    #import spacy stuff\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    tr_tp_st = [[], [], [], []]\n",
    "\n",
    "    #read fox information, this is where ****file names**** and directories should be edited\n",
    "    gl = pd.read_csv(\"../Data/foxGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"../Data/fox_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Sentence'] = lines['Line'].apply(lambda x: [str(sent) for sent in nlp(x).sents])\n",
    "            sentences = lines.explode('Sentence')\n",
    "            sentences = sentences.dropna(subset = ['Sentence'])\n",
    "            sentences['previous'] = sentences['Sentence'].shift(1)\n",
    "\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                #excluding unrelevant entities\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            print(words)\n",
    "            words = words[0:5]\n",
    "            for w in words:\n",
    "                #try:\n",
    "                    #create df of all sentences including word\n",
    "                    w1 = w[0]\n",
    "                    wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n",
    "                    wdf = wdf.reset_index()\n",
    "                    if len(wdf) > 2: #need at least three sentences to be added as an entity\n",
    "                        w_list.append(w1)\n",
    "                        sentence_list = list(wdf.index)\n",
    "                        if len(wdf) > 10:\n",
    "                            sentence_list = sample(sentence_list,10)\n",
    "                        else:\n",
    "                            pass\n",
    "                        sent_list = []\n",
    "                        #taking average (llm determined) sentiment from up to ten sentences with each entity\n",
    "                        #for i, r in wdf.iterrows():\n",
    "                            #if i < 10 and len(r['Line']) > i:\n",
    "                        for i in sentence_list:\n",
    "                                try:\n",
    "                                    r = wdf.loc[i,:]\n",
    "                                    s = r['Sentence']\n",
    "                                    print(s)#r['Sentence'])\n",
    "                                    print(w1)\n",
    "                                    print(f\"Previous: {r['previous']}\")\n",
    "\n",
    "                                    response = openai.ChatCompletion.create(\n",
    "                                        engine=\"ACI_GPT-4\",\n",
    "                                        messages = [{\"role\":\"user\",\"content\":stance_prompt.format(subject = w1, statement= s, previous = r['previous'])}],\n",
    "                                        temperature=0.7,\n",
    "                                        max_tokens=800,\n",
    "                                        top_p=0.95,\n",
    "                                        frequency_penalty=0,\n",
    "                                        presence_penalty=0,\n",
    "                                        stop=None)\n",
    "                                    stance = response.choices[0].message['content']\n",
    "                                    #stance = llm(stance_prompt.format(subject = w1, statement=r['Line']))\n",
    "\n",
    "                                    #mapping llm output to int and storing\n",
    "                                    def stance_value(sentence):  \n",
    "                                        if 'NEGATIVE' in sentence:  \n",
    "                                            return -1  \n",
    "                                        elif 'POSITIVE' in sentence:  \n",
    "                                            return 1  \n",
    "                                        elif 'NEUTRAL' in sentence:  \n",
    "                                            return 0  \n",
    "                                        else:  \n",
    "                                            return \"No sentiment found in the sentence\"  \n",
    "                                    stance1 = stance_value(stance)\n",
    "                                    sent_list.append(stance1)\n",
    "                                    print(stance)\n",
    "                                    print(stance1)\n",
    "                                    llm_df.loc[len(llm_df)] = [w1,s,stance]\n",
    "                                    time.sleep(1.5)\n",
    "                                except Exception as error:\n",
    "                                    print(\"Error:\", error)\n",
    "                                    time.sleep(2)\n",
    "\n",
    "                        word_sentiment_list.append(sent_list)\n",
    "                #except Exception as error:\n",
    "                #    print(\"Error:\", error)\n",
    "\n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "    return tr_tp_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Trump', 35), ('Comey', 28), ('Democrats', 27), ('FBI', 17), ('Mueller', 10), ('American', 9), ('IG', 8), ('Angle', 7), ('Richman', 6), ('Washington', 4), ('Congress', 4), ('Democrat', 4), ('FISA', 4), ('Americans', 4), ('House', 3), ('Russian', 3), ('America', 3), ('Obama', 3), ('Capitol Hill', 3), ('Republicans', 3), ('Clinton', 3), ('Russia', 2), ('USMCA', 2), ('Laura', 2), ('Senate', 2), ('New York', 2), ('California', 2), ('Ukraine', 2), ('Georgia', 2), ('Donald Trump', 2), ('McCabe', 2), ('Steele', 2), ('the United States', 2), ('U.S.', 2), ('Gordon', 2), ('Robert Mueller', 2), ('Barr', 2), ('Chelsea Clinton', 2), ('ho-ho', 2), ('Australia', 2), ('Armie Hammer', 2), ('HANNITY', 1), ('New Year', 1), ('Laura Ingraham', 1), (\"Bob Mueller's\", 1), (\"Michael Horowitz's\", 1), ('Hurricane Dorian', 1), ('Bahamas', 1), ('Notre-Dame Cathedral', 1), ('China', 1), ('Chicago', 1), ('the New York-', 1), ('Acela', 1), ('State Department', 1), ('the Clinton Foundation', 1), ('Allan Lichtman', 1), ('Hillary', 1), ('justice.(END VIDEO CLIP', 1), ('the Electoral College', 1), ('The Electoral College', 1), ('South Carolina', 1), ('Michigan', 1), ('Cory Booker', 1), ('Stacey Abrams', 1), ('Abrams', 1), ('END VIDEO CLIP', 1), ('Democratic', 1), ('U.N.', 1), ('the World Trade Organization', 1), ('the Trans-Pacific Partnership', 1), ('Paris', 1), ('Nancy-Ocasio-Pelosi', 1), ('UNIDENTIFIED', 1), ('Republican', 1), ('a mob.(END VIDEO CLIP', 1), ('White supremacists', 1), ('Biden', 1), ('lynching--(END VIDEO CLIP', 1), ('Indian', 1), ('Virginia', 1), ('Ralph Northam', 1), ('Justin Trudeau', 1), ('the Free Speech Movement', 1), ('Mark Zuckerberg', 1), ('democracy--', 1), ('for--(CROSSTALK', 1), ('ACLU', 1), ('Brett Kavanaugh', 1), ('Zelensky', 1), ('Mick Mulvaney', 1), ('the White House', 1), ('Constitution', 1), ('Hill', 1), ('Bill Clinton', 1), ('NAFTA', 1), ('James Comey', 1), ('C.S. Lewis', 1), ('Jim Comey', 1), ('The Inspector General', 1), ('Department of Justice', 1), ('Clapper', 1), ('Brennan', 1), ('Strzok', 1), ('Counsel', 1), ('Trump Tower', 1), ('CNN', 1), ('Obama Thursday', 1), ('Russians', 1), ('him.(END VIDEO CLIP', 1), ('Daniel Richman', 1), ('Trump-Russia', 1), ('Jim', 1), ('Page 51', 1), ('Bret Baier', 1), ('Page 52', 1), (\"Comey's\", 1), ('Jimmy', 1), ('AG', 1), ('Supreme Court', 1), ('Aishah Hasnie', 1), ('Baghdad', 1), ('Iraqi', 1), ('Pentagon', 1), ('Mideast', 1), ('State', 1), (\"Mike Pompeo's\", 1), ('The State Department', 1), ('Pompeo', 1), ('Nick Gordon', 1), ('Kristina Brown', 1), ('Brown', 1), ('Whitney Houston', 1), ('Bobby Brown', 1), ('Ron', 1), ('emotion!(END VIDEO CLIP', 1), ('the United States of America.(APPLAUSE)(END VIDEO CLIP', 1), ('David Kessler', 1), ('Elisabeth Kubler-Ross', 1), ('African-American', 1), ('Bargaining', 1), ('United States', 1), ('Bob Mueller', 1), ('Rebecca Dudley', 1), ('The New York Times', 1), ('the Mueller Report', 1), ('Southern District', 1), ('the Federalist Papers', 1), ('the University of Illinois at Urbana-Champaign', 1), ('the Covington Catholic', 1), ('Nick Sandmann', 1), ('NYU', 1), ('New Zealand', 1), ('entitled.(BEGIN VIDEO CLIP', 1), ('us.(END VIDEO CLIP', 1), ('Nancy Pelosi', 1), ('Hollywood', 1), ('Generation E.', 1), ('Angles', 1), ('Document', 1)]\n",
      "Well, deep state actors, they looked the other way as Comey abused his position, all the privileges and power of the FBI in a planned effort to destroy the man he had utter disdain for, Donald Trump.\n",
      "Trump\n",
      "Previous: Comey along with Clapper, Brennan, Strzok, and McCabe, and perhaps others, fancied themselves as Trump-resisting superheroes, self-appointed saviors of truth, justice and the American way.\n",
      "NEGATIVE\n",
      "-1\n",
      "The Democrats are just only too happy to forsake formally core issues and beliefs if they believe it will bring down Donald Trump.\n",
      "Trump\n",
      "Previous: The party that was once anti-war rattles against Trump's bringing our troops home, home from deployments that were supposed to last for months, not for years.\n",
      "NEGATIVE\n",
      "-1\n",
      "After anger comes perhaps grief's most embarrassing stage, where they try to convince themselves that up is down, black is white, and Trump is guilty.\n",
      "Trump\n",
      "Previous: You spent two years living in a fantasy world of your own making.\n",
      "NEGATIVE\n",
      "-1\n",
      "They think that our electoral system that it even allowed Trump to win just confirmed in their minds that the system has to be scrapped altogether.(BEGIN VIDEO CLIP)\n",
      "Trump\n",
      "Previous: They think we're a nation founded by evil white men from whom we have basically nothing to learn.\n",
      "NEGATIVE\n",
      "-1\n",
      "He was a deity, a messiah-like figure who would deliver them from the evil of Trump and wash away the sin of the 2016 election.\n",
      "Trump\n",
      "Previous:  You see, Mueller wasn't just a special prosecutor to Democrats.\n",
      "NEGATIVE\n",
      "-1\n",
      "Because they see impeachment as a way to punish not only Trump, but us.\n",
      "Trump\n",
      "Previous: Oh, boy, are they fine!\n",
      "NEGATIVE\n",
      "-1\n",
      "This is why they cannot let Trump's record on the economy or in  foreign policy stand on its own.\n",
      "Trump\n",
      "Previous: They don't trust them in places like South Carolina or Michigan.\n",
      "NEGATIVE\n",
      "-1\n",
      "Now, they are not just angry, the Democrats, with Trump or his policies.\n",
      "Trump\n",
      "Previous:  Well, the key to understanding the current impeachment mania is anger.\n",
      "NEGATIVE\n",
      "-1\n",
      "The IG report tells us that Comey meticulously planned exactly how he would inform President-elect Trump of the salacious details contained in that Steele dossier.\n",
      "Trump\n",
      "Previous: They don't like that one bit.\n",
      "NEUTRAL\n",
      "0\n",
      "So after briefing Trump about the dossier at Trump Tower, Comey started drafting a memo to memorialize the encounter during a car ride back to the office.\n",
      "Trump\n",
      "Previous: He colluded with McCabe, the FBI General Counsel and others in advance to plan their strategy to set up the President-elect.\n",
      "NEUTRAL\n",
      "0\n",
      "But if Comey manages to escape prosecution for conduct that his underlings would be charged for, then the damage he did to the FBI will extend to our belief in the treasured words carved into the Supreme Court building -- equal justice under the law.\n",
      "Comey\n",
      "Previous: Big no-no.\n",
      "NEGATIVE\n",
      "-1\n",
      "\"Comey had several other lawful options available to him to advocate for the appointment of a special counsel, which he told us was his goal in making the disclosure.\n",
      "Comey\n",
      "Previous: This is from Page 61 of the IG report.\n",
      "NEUTRAL\n",
      "0\n",
      "\"Now, you get the sense that when Director Comey entered the FBI building, it became like the wardrobe in that famous C.S. Lewis book.\n",
      "Comey\n",
      "Previous: That's the focus of tonight's \"Angle.\n",
      "NEUTRAL\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m months \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJanuary\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m#['January','February','March','April','May','June','July','August','September','October','November','December']\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m months:\n\u001b[1;32m----> 4\u001b[0m     tr_tp_st \u001b[39m=\u001b[39m transcript_cleaning(m)\n\u001b[0;32m      5\u001b[0m     tdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mzip\u001b[39m(tr_tp_st[\u001b[39m0\u001b[39m], tr_tp_st[\u001b[39m1\u001b[39m], tr_tp_st[\u001b[39m2\u001b[39m], tr_tp_st[\u001b[39m3\u001b[39m]), \n\u001b[0;32m      6\u001b[0m                        columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mTranscript\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSentiment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProgram\u001b[39m\u001b[39m\"\u001b[39m],)\n\u001b[0;32m      7\u001b[0m     tdf\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mprocessed_transcripts_llm_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2020.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 136\u001b[0m, in \u001b[0;36mtranscript_cleaning\u001b[1;34m(month)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[39mprint\u001b[39m(stance1)\n\u001b[0;32m    135\u001b[0m     llm_df\u001b[39m.\u001b[39mloc[\u001b[39mlen\u001b[39m(llm_df)] \u001b[39m=\u001b[39m [w1,s,stance]\n\u001b[1;32m--> 136\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1.5\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    138\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError:\u001b[39m\u001b[39m\"\u001b[39m, error)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save out processed and topic modeled data\n",
    "months = ['January']#['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "for m in months:\n",
    "    tr_tp_st = transcript_cleaning(m)\n",
    "    tdf = pd.DataFrame(zip(tr_tp_st[0], tr_tp_st[1], tr_tp_st[2], tr_tp_st[3]), \n",
    "                       columns=[\"Transcript\", \"Topic\", \"Sentiment\", \"Program\"],)\n",
    "    tdf.to_csv('processed_transcripts_llm_' + m + '2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative transcrip sorting (using nouns and lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcript_cleaning(month):\n",
    "    #import spacy stuff\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    tr_tp_st = [[], [], [], []]\n",
    "\n",
    "    gl = pd.read_csv(\"foxGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"fox_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Line'] = lines['Line'].apply(lambda x: x.lower())\n",
    "            #lines[\"VADER\"] = lines[\"Line\"].apply(\n",
    "            #    lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            #)\n",
    "\n",
    "            #getting most used nouns\n",
    "            \"\"\"word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            tokens = nltk.word_tokenize(raw_text)\n",
    "            words = Counter(tokens).most_common()\n",
    "            words1 = []\n",
    "            i = 0\n",
    "            while len(words1) < 15 and i < len(words):\n",
    "                w = words[i][0]\n",
    "                #print(w)\n",
    "                tag =  nltk.pos_tag([w])\n",
    "                t = tag[0][1] \n",
    "                if t in ['NN','NNS','NNP','NNPS','FW','SYM'] and w not in ['video','clip','i','end','begin']: #or t == \"FW\":\n",
    "                    words1.append(w)\n",
    "                i += 1\"\"\"\n",
    "            \n",
    "            #getting most used known entities\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            words = words[0:15]\n",
    "\n",
    "           #print(len(words))\n",
    "            for w in words:\n",
    "                #try:\n",
    "                    w1 = w[0]\n",
    "                    #print(w1)\n",
    "                    wdf = lines[lines['Line'].astype(str).str.contains(str(w1))]\n",
    "                    wdf = wdf.reset_index()\n",
    "                    print(len(wdf))\n",
    "                    w_list.append(w1)\n",
    "                    sent_list = []\n",
    "                    for i, r in wdf.iterrows():\n",
    "                        if i < 5:\n",
    "                            #r = wdf[i]\n",
    "                            print(r['Line'])\n",
    "                            print(w1)\n",
    "                            stance = llm(stance_prompt.format(subject = w1, statement=r['Line']))\n",
    "                            d = {'AGAINST': -1, 'NEUTRAL': 0, 'FOR': 1}\n",
    "                            stance1 = d[stance]\n",
    "                            sent_list.append(stance1)\n",
    "                            print(stance)\n",
    "                            print(stance1)\n",
    "                    word_sentiment_list.append(sent_list)\n",
    "                #except:\n",
    "                #    pass\n",
    "\n",
    "                \n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "\n",
    "    return tr_tp_st"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
