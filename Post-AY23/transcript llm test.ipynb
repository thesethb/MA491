{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba in /usr/local/lib/python3.10/dist-packages (0.11.2)\n",
      "Requirement already satisfied: coverage in /usr/local/lib/python3.10/dist-packages (from mamba) (7.2.3)\n",
      "Requirement already satisfied: clint in /usr/local/lib/python3.10/dist-packages (from mamba) (0.5.1)\n",
      "Requirement already satisfied: args in /usr/local/lib/python3.10/dist-packages (from clint->mamba) (0.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "/bin/bash: line 1: mamba: command not found\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.27.4)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (15.0.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-network in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from scikit-network) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from scikit-network) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/rwalk/gsdmm.git\n",
      "  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-ezifjbpr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-ezifjbpr\n",
      "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gsdmm==0.1) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-04-22 17:35:38.249497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 17:35:38.748793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-22 17:35:39.451063: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-22 17:35:39.451095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 4c154600bf22\n",
      "2023-04-22 17:35:39.451102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 4c154600bf22\n",
      "2023-04-22 17:35:39.451139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.105.17\n",
      "2023-04-22 17:35:39.451158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.105.17\n",
      "2023-04-22 17:35:39.451166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.105.17\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (59.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Package installations to work on WIRE\n",
    "\n",
    "! pip install mamba\n",
    "! mamba install gensim openai -y\n",
    "! mamba install -c anaconda nltk -y\n",
    "! mamba install -c conda-forge spacy -y\n",
    "! mamba install -c conda-forge pyldavis -y\n",
    "! mamba install -c conda-forge pypdf2 -y\n",
    "! pip install transformers sentence-transformers\n",
    "! pip install scikit-network\n",
    "! pip install git+https://github.com/rwalk/gsdmm.git\n",
    "! pip install gensim\n",
    "\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Seth Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Seth\n",
      "[nltk_data]     Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import pickle\n",
    "#import sknetwork as skn\n",
    "from random import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"names\", \"stopwords\", \"state_union\", \"twitter_samples\", \"movie_reviews\", \"averaged_perceptron_tagger\", \"vader_lexicon\", \"punkt\", \"wordnet\"])\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "# set seed for reproducibility\n",
    "# np.random.seed(493)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Seth Benson\\AppData\\Roaming\\Python\\Python311\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import os, re, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from hdbscan import HDBSCAN\n",
    "import umap\n",
    "import openai\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.representation import LangChain\n",
    "from bertopic.representation import TextGeneration\n",
    "\n",
    "from sentence_transformers  import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain, HuggingFacePipeline, FewShotPromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ce20bd32734c2aa15ec3652668cd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the LLM to use, Langchain verison\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"declare-lab/flan-alpaca-gpt4-xl\", task = 'text2text-generation', device=-1,\n",
    "                                      model_kwargs={\"max_length\":500, \"no_repeat_ngram_size\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generic prompt template\n",
    "\n",
    "template = '''\n",
    "The following is a statement from a cable news transcript. Please classify whether the statement is POSITIVE, NEUTRAL, or NEGATIVE towards \"{subject}\". Only return the stance.\n",
    "\n",
    "statement: {statement}\n",
    "'''\n",
    "\n",
    "stance_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\",\"statement\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Transcript-Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_cleaning(month):\n",
    "    #import spacy stuff\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    #set up llm df\n",
    "    llm_df = pd.DataFrame(columns = ['subject','sentence','output'])\n",
    "\n",
    "    tr_tp_st = [[], [], [], []]\n",
    "\n",
    "    gl = pd.read_csv(\"foxGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"fox_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Sentence'] = lines['Line'].apply(lambda x: [str(sent) for sent in nlp(x).sents])\n",
    "            sentences = lines.explode('Sentence')\n",
    "            sentences = sentences.dropna(subset = ['Sentence'])\n",
    "\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                #excluding unrelevant entities\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            words = words[0:5]\n",
    "            for w in words:\n",
    "                    #try:\n",
    "                        w1 = w[0]\n",
    "                        wdf = sentences[sentences['Sentence'].astype(str).str.contains(str(w1))]\n",
    "                        wdf = wdf.reset_index()\n",
    "                        if len(wdf > 2): #need at least three sentences to be added as an entity\n",
    "                            w_list.append(w1)\n",
    "                            sent_list = []\n",
    "                            #taking average (llm determined) sentiment from up to ten sentences with each entity\n",
    "                            for i, r in wdf.iterrows():\n",
    "                                if i < 10 and len(r['Line']) > 10:\n",
    "                                    print(r['Sentence'])\n",
    "                                    print(w1)\n",
    "                                    stance = llm(stance_prompt.format(subject = w1, statement=r['Line']))\n",
    "                                    d = {'NEGATIVE': -1, 'NEUTRAL': 0, 'POSITIVE': 1}\n",
    "                                    stance1 = d[stance]\n",
    "                                    sent_list.append(stance1)\n",
    "                                    print(stance)\n",
    "                                    print(stance1)\n",
    "                                    llm_df.loc[len(llm_df)] = [w1,r['Line'],stance]\n",
    "                            \n",
    "                            word_sentiment_list.append(sent_list)\n",
    "                    #except:\n",
    "                    #    print('error with the contains line')\n",
    "\n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "    return tr_tp_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      " We're going to look first live at Selma, Alabama, as we remember a civil rights icon, John Lewis, known as the conscience of Congress.\n",
      "John Lewis\n",
      "POSITIVE\n",
      "1\n",
      "Brown chapel AME church, John Lewis' family is beginning to arrive and those who will pay their respects today as well.\n",
      "John Lewis\n",
      "POSITIVE\n",
      "1\n",
      " And from here we will go back, this is Selma, Alabama, on March 1st, as you see Congressman John Lewis there.\n",
      "John Lewis\n",
      "POSITIVE\n",
      "1\n",
      "Thank you for Congressman John Lewis.\n",
      "John Lewis\n",
      "POSITIVE\n",
      "1\n",
      " So as they have wrapped up the very short ceremony at Brown Chapel AME church in Selma, Alabama, they're getting ready to do the ceremonial crossing with Congressman John Lewis carried by a caisson across that bridge.\n",
      "John Lewis\n",
      "POSITIVE\n",
      "1\n",
      "90\n",
      " We're going to look first live at Selma, Alabama, as we remember a civil rights icon, John Lewis, known as the conscience of Congress.\n",
      "John\n",
      "POSITIVE\n",
      "1\n",
      "Brown chapel AME church, John Lewis' family is beginning to arrive and those who will pay their respects today as well.\n",
      "John\n",
      "POSITIVE\n",
      "1\n",
      " And from here we will go back, this is Selma, Alabama, on March 1st, as you see Congressman John Lewis there.\n",
      "John\n",
      "POSITIVE\n",
      "1\n",
      "On bloody Sunday in 1965, John was confronted by Alabama state troopers and their dogs.\n",
      "John\n",
      "POSITIVE\n",
      "1\n",
      "But John was determined to fight for equality and justice, putting his own life on the line in the service of others and a brighter future for everyone.\n",
      "John\n",
      "POSITIVE\n",
      "1\n",
      "15\n",
      " We're going to look first live at Selma, Alabama, as we remember a civil rights icon, John Lewis, known as the conscience of Congress.\n",
      "Alabama\n",
      "POSITIVE\n",
      "1\n",
      "This is the Alabama ground where he first faced one of his most dangerous confrontations in America's struggle for freedoms for everyone.\n",
      "Alabama\n",
      "POSITIVE\n",
      "1\n",
      " And from here we will go back, this is Selma, Alabama, on March 1st, as you see Congressman John Lewis there.\n",
      "Alabama\n",
      "POSITIVE\n",
      "1\n",
      "On bloody Sunday in 1965, John was confronted by Alabama state troopers and their dogs.\n",
      "Alabama\n",
      "POSITIVE\n",
      "1\n",
      "It's poetic justice that this time Alabama state troopers will see John to his safety.\n",
      "Alabama\n",
      "POSITIVE\n",
      "1\n",
      "20\n",
      "This is the Alabama ground where he first faced one of his most dangerous confrontations in America's struggle for freedoms for everyone.\n",
      "America\n",
      "POSITIVE\n",
      "1\n",
      "This will be an hour of emotional memories, as you might imagine, of the life that was extraordinary of an American.\n",
      "America\n",
      "POSITIVE\n",
      "1\n",
      " -- Selma, help free and liberate not just American south.\n",
      "America\n",
      "POSITIVE\n",
      "1\n",
      "As he takes his final march, that final crossing, John bridged the gaps that so often divided us, our political parties working everyday for a more just and equitable America.\n",
      "America\n",
      "POSITIVE\n",
      "1\n",
      "March 7th, 1965, a route across the bridge 1200 feet would begin to change America.\n",
      "America\n",
      "POSITIVE\n",
      "1\n",
      "11\n",
      " We're going to look first live at Selma, Alabama, as we remember a civil rights icon, John Lewis, known as the conscience of Congress.\n",
      "Selma\n",
      "POSITIVE\n",
      "1\n",
      " -- Selma, help free and liberate not just American south.\n",
      "Selma\n",
      "POSITIVE\n",
      "1\n",
      " And from here we will go back, this is Selma, Alabama, on March 1st, as you see Congressman John Lewis there.\n",
      "Selma\n",
      "POSITIVE\n",
      "1\n",
      "Let's watch the church service in Selma.(BEGIN VIDEO CLIP)\n",
      "Selma\n",
      "POSITIVE\n",
      "1\n",
      "He will forever change Selma and this nation.\n",
      "Selma\n",
      "POSITIVE\n",
      "1\n",
      "15\n",
      "I'm Juan Williams, along with the Jesse Watters, Greg President Trump set to make a major announcement just moments from now.\n",
      "Juan\n",
      "POSITIVE\n",
      "1\n",
      "Juan, The Five is not like an old video game, where if you log off, all of your progress isn't saved.keep it locked up or open it up all the way.\n",
      "Juan\n",
      "POSITIVE\n",
      "1\n",
      "It's not a place I want to be, Juan.\n",
      "Juan\n",
      "NEGATIVE\n",
      "-1\n",
      "The  way you framed it, Juan, was incorrect.\n",
      "Juan\n",
      "NEGATIVE\n",
      "-1\n",
      " I don't want to send my kids back to school as normal either, Juan.\n",
      "Juan\n",
      "POSITIVE\n",
      "1\n",
      "17\n",
      "I'm Juan Williams, along with the Jesse Watters, Greg President Trump set to make a major announcement just moments from now.\n",
      "Trump\n",
      "POSITIVE\n",
      "1\n",
      "So Shannon, the president's going to come out in a few minutes to announce some White House guidelines for school reopening, now, as you recall last week, the CDC, the Centers for Disease Control, came out with their own set of guidelines, the White House, the Trump White House, objected apparently dealing them as too strict.\n",
      "Trump\n",
      "NEUTRAL\n",
      "0\n",
      " Because Baron is going to get tested when he comes back and forth, and Donald Trump is going to get tested, so that doesn't matter.\n",
      "Trump\n",
      "NEUTRAL\n",
      "0\n",
      "There's a lot of questions this year about whether pollsters will get it right between the race between Joe Biden and President Trump.\n",
      "Trump\n",
      "NEUTRAL\n",
      "0\n",
      "The problem is that we live in a very politically polarized era, the Trump era.\n",
      "Trump\n",
      "NEGATIVE\n",
      "-1\n",
      "10\n",
      " -- uniting the country in our fight against the China virus.\n",
      "China\n",
      "NEGATIVE\n",
      "-1\n",
      "And I said there's nothing more important in our country than keeping our people safe, whether that's from the China virus or the radical left mob that you see in Portland.\n",
      "China\n",
      "NEGATIVE\n",
      "-1\n",
      "Our goal is to protect our teachers and students from the China virus while ensuring that families with high risk factors can continue to participate from home.\n",
      "China\n",
      "NEGATIVE\n",
      "-1\n",
      "Fortunately, the data shows that children are lower risk from the China virus very substantially.\n",
      "China\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m months \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJuly\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m#['January','February','March','April','May','June','July','August','September','October','November','December']\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m months:\n\u001b[1;32m----> 4\u001b[0m     tr_tp_st \u001b[39m=\u001b[39m transcript_cleaning(m)\n\u001b[0;32m      5\u001b[0m     tdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mzip\u001b[39m(tr_tp_st[\u001b[39m0\u001b[39m], tr_tp_st[\u001b[39m1\u001b[39m], tr_tp_st[\u001b[39m2\u001b[39m], tr_tp_st[\u001b[39m3\u001b[39m]), \n\u001b[0;32m      6\u001b[0m                        columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mTranscript\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSentiment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProgram\u001b[39m\u001b[39m\"\u001b[39m],)\n",
      "Cell \u001b[1;32mIn[51], line 94\u001b[0m, in \u001b[0;36mtranscript_cleaning\u001b[1;34m(month)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mprint\u001b[39m(r[\u001b[39m'\u001b[39m\u001b[39mSentence\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     93\u001b[0m \u001b[39mprint\u001b[39m(w1)\n\u001b[1;32m---> 94\u001b[0m stance \u001b[39m=\u001b[39m llm(stance_prompt\u001b[39m.\u001b[39mformat(subject \u001b[39m=\u001b[39m w1, statement\u001b[39m=\u001b[39mr[\u001b[39m'\u001b[39m\u001b[39mLine\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m     95\u001b[0m \u001b[39m#d = {'AGAINST': -1, 'NEUTRAL': 0, 'FOR': 1}\u001b[39;00m\n\u001b[0;32m     96\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mNEGATIVE\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNEUTRAL\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPOSITIVE\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:786\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    780\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    781\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    783\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    784\u001b[0m     )\n\u001b[0;32m    785\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 786\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    787\u001b[0m         [prompt],\n\u001b[0;32m    788\u001b[0m         stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    789\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    790\u001b[0m         tags\u001b[39m=\u001b[39mtags,\n\u001b[0;32m    791\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m    792\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    793\u001b[0m     )\n\u001b[0;32m    794\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    795\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    796\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:582\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         )\n\u001b[0;32m    576\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    577\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    578\u001b[0m             dumpd(\u001b[39mself\u001b[39m), [prompt], invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[0;32m    579\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    580\u001b[0m         \u001b[39mfor\u001b[39;00m callback_manager, prompt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    581\u001b[0m     ]\n\u001b[1;32m--> 582\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_helper(\n\u001b[0;32m    583\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:488\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    487\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    489\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    490\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:475\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    467\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    472\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    473\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 475\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    476\u001b[0m                 prompts,\n\u001b[0;32m    477\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    478\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    479\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    480\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    481\u001b[0m             )\n\u001b[0;32m    482\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    483\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    484\u001b[0m         )\n\u001b[0;32m    485\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    486\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:961\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    959\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m    960\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 961\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    962\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    963\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[0;32m    966\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\huggingface_pipeline.py:168\u001b[0m, in \u001b[0;36mHuggingFacePipeline._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m    162\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    163\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    167\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 168\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline(prompt)\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    170\u001b[0m         \u001b[39m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m         text \u001b[39m=\u001b[39m response[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mlen\u001b[39m(prompt) :]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    170\u001b[0m     ):\n\u001b[0;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\base.py:1122\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1115\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         )\n\u001b[0;32m   1120\u001b[0m     )\n\u001b[0;32m   1121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1122\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1128\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1129\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1130\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1131\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\base.py:1028\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1027\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1028\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1029\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1030\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1345\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1338\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1339\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1340\u001b[0m         )\n\u001b[0;32m   1342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1343\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1344\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1345\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m   1346\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:644\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    642\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    643\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 644\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    646\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1094\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1082\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1083\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m     )\n\u001b[0;32m   1093\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1094\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1095\u001b[0m         hidden_states,\n\u001b[0;32m   1096\u001b[0m         attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1097\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m   1098\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1099\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1100\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39mencoder_decoder_position_bias,\n\u001b[0;32m   1101\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m   1102\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[0;32m   1103\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m   1104\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1105\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1106\u001b[0m     )\n\u001b[0;32m   1108\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py:694\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer[\u001b[39m0\u001b[39m](\n\u001b[0;32m    695\u001b[0m     hidden_states,\n\u001b[0;32m    696\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    697\u001b[0m     position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m    698\u001b[0m     layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    699\u001b[0m     past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    700\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    701\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    702\u001b[0m )\n\u001b[0;32m    703\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    704\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    591\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    592\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    599\u001b[0m ):\n\u001b[0;32m    600\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 601\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSelfAttention(\n\u001b[0;32m    602\u001b[0m         normed_hidden_states,\n\u001b[0;32m    603\u001b[0m         mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    604\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m    605\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    606\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m    607\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    608\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    611\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py:561\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    558\u001b[0m     position_bias_masked \u001b[39m=\u001b[39m position_bias\n\u001b[0;32m    560\u001b[0m scores \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_bias_masked\n\u001b[1;32m--> 561\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores\u001b[39m.\u001b[39mfloat(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype_as(\n\u001b[0;32m    562\u001b[0m     scores\n\u001b[0;32m    563\u001b[0m )  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[0;32m    564\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(\n\u001b[0;32m    565\u001b[0m     attn_weights, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[0;32m    566\u001b[0m )  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1842\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim)\n\u001b[0;32m   1844\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save out processed and topic modeled data\n",
    "months = ['July']#['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "for m in months:\n",
    "    tr_tp_st = transcript_cleaning(m)\n",
    "    tdf = pd.DataFrame(zip(tr_tp_st[0], tr_tp_st[1], tr_tp_st[2], tr_tp_st[3]), \n",
    "                       columns=[\"Transcript\", \"Topic\", \"Sentiment\", \"Program\"],)\n",
    "    #with open('v9 processed_transcripts_' + m + '2020.pkl', 'wb') as file:\n",
    "    #    pickle.dump(tr_tp_st, file)\n",
    "    tdf.to_csv('processed_transcripts_llm_' + m + '2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative transcrip sorting (using nouns and lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcript_cleaning(month):\n",
    "    #import spacy stuff\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    tr_tp_st = [[], [], [], []]\n",
    "\n",
    "    gl = pd.read_csv(\"foxGuestList.csv\", encoding=\"windows-1252\")\n",
    "    fox = open(\"fox_text.txt\", \"r\", encoding=\"windows-1252\")\n",
    "    fox = fox.readlines()\n",
    "    for l in range(len(gl)):\n",
    "        #######################################\n",
    "        ##Turning transcript into df of lines##\n",
    "        #######################################\n",
    "        lines = pd.DataFrame(\n",
    "            columns=[\"Date\", \"Start_Hour\", \"Program\", \"Title\", \"Speaker\", \"Line\"]\n",
    "        )\n",
    "        # print(lines)\n",
    "        transcript = fox[l]\n",
    "        meta = gl.iloc[l, :]\n",
    "        dt = meta[\"Date\"]\n",
    "        if month in str(dt):\n",
    "            if randint(1, 100) <= 5:\n",
    "                print(l)\n",
    "            chunks = transcript.split(\"|\")\n",
    "            line = \"\"\n",
    "            spkr = \"\"\n",
    "            for c in chunks:\n",
    "                if (\n",
    "                    \"Page\" not in c[0:10]\n",
    "                    and \"....\" not in c\n",
    "                    and c[0:15] not in meta[\"Title\"]\n",
    "                ):\n",
    "                    # print(c)\n",
    "                    if \":\" in c:\n",
    "                        c = c.split(\":\")\n",
    "                        if str.isupper(c[0]):\n",
    "                            lines.loc[len(lines)] = [\n",
    "                                meta[\"Date\"],\n",
    "                                meta[\"Start_Hour\"],\n",
    "                                meta[\"Program\"],\n",
    "                                meta[\"Title\"],\n",
    "                                spkr,\n",
    "                                line,\n",
    "                            ]\n",
    "                            # print(spkr + ':' + line)\n",
    "                            spkr = c[0]\n",
    "                            line = c[1]\n",
    "                    else:\n",
    "                        line += c\n",
    "            lines.loc[len(lines)] = [\n",
    "                meta[\"Date\"],\n",
    "                meta[\"Start_Hour\"],\n",
    "                meta[\"Program\"],\n",
    "                meta[\"Title\"],\n",
    "                spkr,\n",
    "                line,\n",
    "            ]\n",
    "\n",
    "            #########################\n",
    "            ##Keyword and Sentiment##\n",
    "            #########################\n",
    "\n",
    "            lines['Line'] = lines['Line'].apply(lambda x: x.lower())\n",
    "            #lines[\"VADER\"] = lines[\"Line\"].apply(\n",
    "            #    lambda x: list(sid.polarity_scores(x).values())[3]\n",
    "            #)\n",
    "\n",
    "            #getting most used nouns\n",
    "            \"\"\"word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            tokens = nltk.word_tokenize(raw_text)\n",
    "            words = Counter(tokens).most_common()\n",
    "            words1 = []\n",
    "            i = 0\n",
    "            while len(words1) < 15 and i < len(words):\n",
    "                w = words[i][0]\n",
    "                #print(w)\n",
    "                tag =  nltk.pos_tag([w])\n",
    "                t = tag[0][1] \n",
    "                if t in ['NN','NNS','NNP','NNPS','FW','SYM'] and w not in ['video','clip','i','end','begin']: #or t == \"FW\":\n",
    "                    words1.append(w)\n",
    "                i += 1\"\"\"\n",
    "            \n",
    "            #getting most used known entities\n",
    "            word_sentiment_list = []\n",
    "            w_list = []\n",
    "            raw_text= ' '.join(lines['Line'])\n",
    "            text1 = NER(raw_text)\n",
    "            words = []\n",
    "            for word in text1.ents:\n",
    "                if word.label_ not in ['TIME','MONEY','PERCENT','DATE','QUANTITY','ORDINAL','CARDINAL']:\n",
    "                    words.append(word.text)\n",
    "            words = Counter(words).most_common()\n",
    "            words = words[0:15]\n",
    "\n",
    "           #print(len(words))\n",
    "            for w in words:\n",
    "                #try:\n",
    "                    w1 = w[0]\n",
    "                    #print(w1)\n",
    "                    wdf = lines[lines['Line'].astype(str).str.contains(str(w1))]\n",
    "                    wdf = wdf.reset_index()\n",
    "                    print(len(wdf))\n",
    "                    w_list.append(w1)\n",
    "                    sent_list = []\n",
    "                    for i, r in wdf.iterrows():\n",
    "                        if i < 5:\n",
    "                            #r = wdf[i]\n",
    "                            print(r['Line'])\n",
    "                            print(w1)\n",
    "                            stance = llm(stance_prompt.format(subject = w1, statement=r['Line']))\n",
    "                            d = {'AGAINST': -1, 'NEUTRAL': 0, 'FOR': 1}\n",
    "                            stance1 = d[stance]\n",
    "                            sent_list.append(stance1)\n",
    "                            print(stance)\n",
    "                            print(stance1)\n",
    "                    word_sentiment_list.append(sent_list)\n",
    "                #except:\n",
    "                #    pass\n",
    "\n",
    "                \n",
    "            ######################\n",
    "            ##Storing Each Topic##\n",
    "            ######################\n",
    "            for w in range(len(w_list)):\n",
    "                if len(word_sentiment_list[w]) > 2:\n",
    "                    tr_tp_st[0].append(l)\n",
    "                    tr_tp_st[1].append(w_list[w])\n",
    "                    tr_tp_st[2].append(np.mean(word_sentiment_list[w]))\n",
    "                    tr_tp_st[3].append(lines[\"Program\"][0])\n",
    "\n",
    "\n",
    "    return tr_tp_st"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
